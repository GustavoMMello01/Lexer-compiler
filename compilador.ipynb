{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Token</h3>\n",
    "<h4>Analisador Lexico - AFD </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KEYWORD = \"<keyword %s>\"\n",
    "T_OP = \"<op %s>\"\n",
    "T_INT = \"<int %s>\"\n",
    "T_STRING = \"<string %s>\"\n",
    "T_IDENTIF = \"<id %s>\"\n",
    "T_SPECIAL = \"<special %s>\"\n",
    "T_PUNCT = \"<punct %s>\"\n",
    "T_DOT = \"<dot>\"\n",
    "T_CONDITIONAL_OP = \"<conditional_op %s>\"\n",
    "T_COMMENT = \"<comment>\"\n",
    "\n",
    "class Token():\n",
    "    def __init__(self, tipo, valor=None):\n",
    "        self.tipo = tipo\n",
    "        self.valor = valor\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Token(tipo={self.tipo}, valor={self.valor})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "def tokenize_line(line, line_number):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(line)\n",
    "\n",
    "    while i < n:\n",
    "        if line[i].isspace():\n",
    "            i += 1\n",
    "        elif line[i] == '#':  # Comment\n",
    "            start = i\n",
    "            while i < n:\n",
    "                i += 1\n",
    "            tokens.append(Token(\"T_COMMENT\", line[start:i]))\n",
    "            break\n",
    "        elif line[i].isalpha():\n",
    "            start = i\n",
    "            while i < n and (line[i].isalnum() or line[i] == '_'):\n",
    "                i += 1\n",
    "            word = line[start:i]\n",
    "            if word in [\"var\", \"func\", \"if\", \"elif\", \"else\", \"return\", \"object\", \"init\"]:\n",
    "                tokens.append(Token(\"T_KEYWORD\", word))\n",
    "            elif word in [\"true\", \"false\", \"null\", \"end\", \"main\"]:\n",
    "                tokens.append(Token(\"T_SPECIAL\", word))\n",
    "            else:\n",
    "                tokens.append(Token(\"T_IDENTIF\", word))\n",
    "        elif line[i].isdigit():\n",
    "            start = i\n",
    "            while i < n and line[i].isdigit():\n",
    "                i += 1\n",
    "            tokens.append(Token(\"T_INT\", line[start:i]))\n",
    "        elif line[i] == '\"':\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < n and line[i] != '\"':\n",
    "                i += 1\n",
    "            if i >= n:\n",
    "                print(f\"Erro: String não fechada na linha {line_number}\")\n",
    "                raise StopExecution\n",
    "            i += 1\n",
    "            tokens.append(Token(\"T_STRING\", line[start:i]))\n",
    "        elif line[i] in \"=<>!+-*/\":\n",
    "            start = i\n",
    "            i += 1\n",
    "            if i < n and line[i] == \"=\":\n",
    "                i += 1\n",
    "            tokens.append(Token(\"T_OP\", line[start:i]))\n",
    "        elif line[i] in \"(),[]{}\":\n",
    "            tokens.append(Token(\"T_PUNCT\", line[i]))\n",
    "            i += 1\n",
    "        elif line[i] in \"?:\":\n",
    "            tokens.append(Token(\"T_CONDITIONAL_OP\", line[i]))\n",
    "            i += 1\n",
    "        elif line[i] == '.':\n",
    "            tokens.append(Token(\"T_DOT\"))\n",
    "            i += 1\n",
    "        else:\n",
    "            print(f\"Erro: Caractere não reconhecido '{line[i]}' na linha {line_number}\")\n",
    "            raise StopExecution\n",
    "    return tokens\n",
    "\n",
    "def tokenize():\n",
    "\n",
    "    try:\n",
    "        token_total = [];\n",
    "        with open('codigo.x', 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        line_number = 0\n",
    "        for line in lines:\n",
    "            line_number += 1\n",
    "            tokens = tokenize_line(line, line_number)\n",
    "            token_total.extend(tokens)\n",
    "            #print(tokens)\n",
    "\n",
    "        return token_total\n",
    "\n",
    "    except StopExecution:\n",
    "        print(\"Execução parada devido a erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analisador Sinatico - Parser </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.current_idx = 0  \n",
    "        self.current_token = self.tokens[0] if tokens else None\n",
    "        self.symbol_table = {}\n",
    "\n",
    "    def get_next_token(self):\n",
    "        self.current_idx += 1\n",
    "        if self.current_idx < len(self.tokens):\n",
    "            self.current_token = self.tokens[self.current_idx]\n",
    "        else:\n",
    "            self.current_token = None\n",
    "\n",
    "    def eat(self, token_type):\n",
    "        if self.current_token.tipo == token_type:\n",
    "            self.get_next_token()\n",
    "        else:\n",
    "            raise Exception(f\"Erro de sintaxe. Esperado: {token_type}. Recebido: {self.current_token.tipo} - Valor: {self.current_token.valor}\")\n",
    "\n",
    "    def factor(self):\n",
    "        \"\"\"factor : INT | IDENTIF | ( expr )\"\"\"\n",
    "        token = self.current_token\n",
    "        if token.tipo == 'T_INT':\n",
    "            self.eat('T_INT')\n",
    "            return int(token.valor)\n",
    "        elif token.tipo == 'T_IDENTIF':\n",
    "            self.eat('T_IDENTIF')\n",
    "            return self.symbol_table.get(token.valor, None) \n",
    "        elif token.tipo == 'T_PUNCT' and token.valor == '(':\n",
    "            self.eat('T_PUNCT')\n",
    "            result = self.expr()\n",
    "            self.eat('T_PUNCT')\n",
    "            return result\n",
    "\n",
    "    def term(self):\n",
    "        \"\"\"term : factor ((MUL | DIV) factor)*\"\"\"\n",
    "        result = self.factor()\n",
    "\n",
    "        while self.current_token is not None and self.current_token.tipo in ('T_OP') and self.current_token.valor in ['*', '/']:\n",
    "            token = self.current_token\n",
    "            if token.valor == '*':\n",
    "                self.eat('T_OP')\n",
    "                result *= self.factor()\n",
    "            elif token.valor == '/':\n",
    "                self.eat('T_OP')\n",
    "                result /= self.factor()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def expr(self):\n",
    "        \"\"\"expr : term ((PLUS | MINUS) term)*\"\"\"\n",
    "        result = self.term()\n",
    "\n",
    "        while self.current_token is not None and self.current_token.tipo in ('T_OP') and self.current_token.valor in ['+', '-']:\n",
    "            token = self.current_token\n",
    "            if token.valor == '+':\n",
    "                self.eat('T_OP')\n",
    "                result += self.term()\n",
    "            elif token.valor == '-':\n",
    "                self.eat('T_OP')\n",
    "                result -= self.term()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def assignment(self):\n",
    "        \"\"\"IDENTIF EQUALS expr\"\"\"\n",
    "        var_name = self.current_token.valor\n",
    "        self.eat('T_IDENTIF')\n",
    "        self.eat('T_OP')\n",
    "        var_value = self.expr()\n",
    "        self.symbol_table[var_name] = var_value\n",
    "    \n",
    "    def conditional_statement(self):\n",
    "        \"\"\" if/elif/else statement \"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir o \"if\" ou \"elif\"\n",
    "        condition = self.expr()  # Avaliar a expressão condicional\n",
    "\n",
    "        if condition:\n",
    "            self.eat('T_PUNCT')  # Consumir o '{'\n",
    "            self.start()  # Processar o bloco de código dentro do if/elif\n",
    "            self.eat('T_PUNCT')  # Consumir o '}'\n",
    "        else:\n",
    "            # pular o bloco de código e possivelmente ir para um \"elif\" ou \"else\"\n",
    "            while self.current_token.tipo != 'T_PUNCT' or self.current_token.valor != '}':\n",
    "                self.eat(self.current_token.tipo)\n",
    "\n",
    "            self.eat('T_PUNCT')  # Consumir o '}'\n",
    "\n",
    "            if self.current_token and self.current_token.valor == \"elif\":\n",
    "                self.conditional_statement()\n",
    "            elif self.current_token and self.current_token.valor == \"else\":\n",
    "                self.eat('T_KEYWORD')  # Consumir o \"else\"\n",
    "                self.eat('T_PUNCT')  # Consumir o '{'\n",
    "                self.start()  # Processar o bloco de código dentro do else\n",
    "                self.eat('T_PUNCT')  # Consumir o '}'\n",
    "\n",
    "    def loop_statement(self):\n",
    "        \"\"\" while statement\"\"\"\n",
    "        \"\"\"TODO Implementar outros loops como o for...\"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir o \"while\"\n",
    "        condition = self.expr()  # Avaliar a expressão condicional\n",
    "\n",
    "        while condition:\n",
    "            self.eat('T_PUNCT')  # Consumir o '{'\n",
    "            self.start()  # Processar o bloco de código dentro do loop\n",
    "            self.eat('T_PUNCT')  # Consumir o '}'\n",
    "\n",
    "    def function_definition(self):\n",
    "        \"\"\" func IDENTIFIER (...) \"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir o \"func\"\n",
    "        func_name = self.current_token.valor\n",
    "        self.eat('T_IDENTIF')\n",
    "        self.eat('T_PUNCT')  # Consumir o '('\n",
    "\n",
    "        # Lista de parâmetros (opcional)\n",
    "        parameters = []\n",
    "        while self.current_token.tipo != 'T_PUNCT' or self.current_token.valor != ')':\n",
    "            parameters.append(self.current_token.valor)\n",
    "            self.eat('T_IDENTIF')\n",
    "\n",
    "            # Se houver uma vírgula, é porque há mais parâmetros\n",
    "            if self.current_token.valor == ',':\n",
    "                self.eat('T_PUNCT')  # Consumir a ','\n",
    "\n",
    "        self.eat('T_PUNCT')  # Consumir o ')'\n",
    "\n",
    "        # Adicionar função à tabela de símbolos (para este exemplo, vamos apenas armazenar o nome)\n",
    "        self.symbol_table[func_name] = {'type': 'function', 'parameters': parameters}\n",
    "\n",
    "        self.eat('T_PUNCT')  # Consumir o '{'\n",
    "        self.start()  # Processar o corpo da função\n",
    "        self.eat('T_PUNCT')  # Consumir o '}'\n",
    "\n",
    "    def object_definition(self):\n",
    "        \"\"\"TODO expandir isso para incluir membros de objeto, métodos, etc.\"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir o \"object\"\n",
    "        object_name = self.current_token.valor\n",
    "        self.eat('T_IDENTIF')\n",
    "\n",
    "        self.symbol_table[object_name] = {'type': 'object'}\n",
    "\n",
    "\n",
    "    def print_command(self):\n",
    "        \"\"\" Trata o comando print \"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir \"print\"\n",
    "        self.eat('T_PUNCT')    # Consumir '('\n",
    "        \n",
    "        # Aqui podemos melhorar para tratar expressões mais complexas dentro do print.\n",
    "        while self.current_token.tipo != 'T_PUNCT' or self.current_token.valor != ')':\n",
    "            if self.current_token.tipo == 'T_STRING':\n",
    "                # Supondo que esteja imprimindo uma string, podemos apenas consumir o token.\n",
    "                self.eat('T_STRING')\n",
    "            else:\n",
    "                self.expr()\n",
    "\n",
    "            if self.current_token.valor == ',':\n",
    "                self.eat('T_PUNCT')  # Consumir ','\n",
    "\n",
    "        self.eat('T_PUNCT')    # Consumir ')'\n",
    "\n",
    "    def variable_declaration(self):\n",
    "        \"\"\" Trata a declaração de variáveis: var IDENTIF = expr \"\"\"\n",
    "        self.eat('T_KEYWORD')  # Consumir \"var\"\n",
    "        var_name = self.current_token.valor\n",
    "        self.eat('T_IDENTIF')\n",
    "        \n",
    "        # Aceitação opcional de espaços ou '=' diretamente\n",
    "        if self.current_token.tipo == 'T_OP' and self.current_token.valor == '=':\n",
    "            self.eat('T_OP')  # Consumir '='\n",
    "            var_value = self.expr()\n",
    "            self.symbol_table[var_name] = var_value\n",
    "    \n",
    "    def string_literal(self):\n",
    "        \"\"\" Trata string literais \"\"\"\n",
    "        return self.current_token.valor \n",
    "\n",
    "    def end_command(self):\n",
    "        \"\"\" Trata o comando end \"\"\"\n",
    "        self.eat('T_KEYWORD')  \n",
    "\n",
    "    def block(self):\n",
    "        \"\"\" Trata blocos de código dentro de chaves \"\"\"\n",
    "        self.eat('T_PUNCT')  # Consumir \"{\"\n",
    "        while self.current_token.valor != '}':\n",
    "            self.start()\n",
    "        self.eat('T_PUNCT')  # Consumir \"}\"\n",
    "\n",
    "    def function_call(self):\n",
    "        \"\"\" Trata chamadas de funções \"\"\"\n",
    "        func_name = self.current_token.valor\n",
    "        self.eat('T_IDENTIF')\n",
    "        self.eat('T_PUNCT')  # Consumir \"(\"\n",
    "        \n",
    "        args = []\n",
    "        while self.current_token.valor != ')':\n",
    "            args.append(self.expr())\n",
    "            if self.current_token.valor == ',':\n",
    "                self.eat('T_PUNCT')  # Consumir \",\"\n",
    "        \n",
    "        self.eat('T_PUNCT')  # Consumir \")\"\n",
    "        # Aqui você pode, por exemplo, tratar a chamada da função.\n",
    "        # No exemplo, estamos apenas passando por ela.\n",
    "\n",
    "    def special_function_definition(self):\n",
    "        \"\"\" Trata funções especiais como 'main' e 'init' \"\"\"\n",
    "        func_name = self.current_token.valor\n",
    "        self.eat('T_SPECIAL')  # Consumir o nome especial (main, init, etc.)\n",
    "        \n",
    "        self.eat('T_PUNCT')  # Consumir o '('\n",
    "\n",
    "        # Para simplicidade, estamos supondo que funções especiais não possuem parâmetros.\n",
    "        # Se elas puderem ter parâmetros, é necessário expandir esta parte.\n",
    "\n",
    "        self.eat('T_PUNCT')  # Consumir o ')'\n",
    "        \n",
    "        # Adicionar função à tabela de símbolos (para este exemplo, vamos apenas armazenar o nome)\n",
    "        self.symbol_table[func_name] = {'type': 'function', 'parameters': []}\n",
    "\n",
    "        self.eat('T_PUNCT')  # Consumir o '{'\n",
    "        self.start()  # Processar o corpo da função\n",
    "        self.eat('T_PUNCT')  # Consumir o '}'\n",
    "\n",
    "    def statement(self):\n",
    "        \"\"\"Parse a single statement based on the current token.\"\"\"\n",
    "        if self.current_token is None:\n",
    "            return\n",
    "        if self.current_token.tipo == 'T_IDENTIF':\n",
    "            lookahead_idx = self.current_idx + 1\n",
    "            if lookahead_idx < len(self.tokens) and self.tokens[lookahead_idx].valor == '(':\n",
    "                self.function_call()\n",
    "            else:\n",
    "                self.assignment()\n",
    "        elif self.current_token.tipo == 'T_KEYWORD':\n",
    "            if self.current_token.valor == \"var\":\n",
    "                self.variable_declaration()\n",
    "            elif self.current_token.valor == \"if\":\n",
    "                self.conditional_statement()\n",
    "            elif self.current_token.valor == \"while\":\n",
    "                self.loop_statement()\n",
    "            elif self.current_token.valor == \"func\":\n",
    "                self.function_definition()\n",
    "            elif self.current_token.valor == \"end\":\n",
    "                self.end_command()\n",
    "            elif self.current_token.valor == \"object\":\n",
    "                self.object_definition()\n",
    "            elif self.current_token.valor == \"print\":\n",
    "                self.print_command()\n",
    "        elif self.current_token.tipo == 'T_STRING':\n",
    "            self.string_literal()\n",
    "            self.eat('T_STRING')\n",
    "        elif self.current_token.tipo == 'T_PUNCT' and self.current_token.valor == '{':\n",
    "            self.block()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "    def error(self):\n",
    "        raise Exception(f\"Erro de sintaxe no token {self.current_token.valor}\")\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\" Entrada do parser \"\"\"\n",
    "        while self.current_token is not None:\n",
    "            if self.current_token.tipo == 'T_SPECIAL':\n",
    "                self.special_function_definition()\n",
    "            else:\n",
    "                self.statement()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compilador</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo codigo2.x ...\n",
      "# ANT\n",
      "func main home(){\n",
      "    var a = 1\n",
      "}\n",
      "\n",
      "Tokenização:\n",
      "OLAAAAAAAAAA  # Bem vindo a essa nova linguagem de prog ANT\n",
      "Tokens:\n",
      "001. Token(tipo=T_COMMENT, valor=# Bem vindo a essa nova linguagem de prog ANT)\n",
      "002. Token(tipo=T_KEYWORD, valor=func)\n",
      "003. Token(tipo=T_SPECIAL, valor=main)\n",
      "004. Token(tipo=T_IDENTIF, valor=home)\n",
      "005. Token(tipo=T_PUNCT, valor=()\n",
      "006. Token(tipo=T_PUNCT, valor=))\n",
      "007. Token(tipo=T_PUNCT, valor={)\n",
      "008. Token(tipo=T_INT, valor=2)\n",
      "009. Token(tipo=T_KEYWORD, valor=var)\n",
      "010. Token(tipo=T_IDENTIF, valor=a)\n",
      "011. Token(tipo=T_OP, valor==)\n",
      "012. Token(tipo=T_INT, valor=1)\n",
      "013. Token(tipo=T_KEYWORD, valor=var)\n",
      "014. Token(tipo=T_IDENTIF, valor=b)\n",
      "015. Token(tipo=T_OP, valor==)\n",
      "016. Token(tipo=T_INT, valor=2)\n",
      "017. Token(tipo=T_KEYWORD, valor=var)\n",
      "018. Token(tipo=T_IDENTIF, valor=nome)\n",
      "019. Token(tipo=T_OP, valor==)\n",
      "020. Token(tipo=T_STRING, valor=\"Joao\")\n",
      "021. Token(tipo=T_KEYWORD, valor=var)\n",
      "022. Token(tipo=T_IDENTIF, valor=c)\n",
      "023. Token(tipo=T_OP, valor==)\n",
      "024. Token(tipo=T_IDENTIF, valor=add)\n",
      "025. Token(tipo=T_PUNCT, valor=()\n",
      "026. Token(tipo=T_IDENTIF, valor=a)\n",
      "027. Token(tipo=T_PUNCT, valor=,)\n",
      "028. Token(tipo=T_IDENTIF, valor=b)\n",
      "029. Token(tipo=T_PUNCT, valor=))\n",
      "030. Token(tipo=T_IDENTIF, valor=print)\n",
      "031. Token(tipo=T_PUNCT, valor=()\n",
      "032. Token(tipo=T_STRING, valor=\"O resultado foi \")\n",
      "033. Token(tipo=T_PUNCT, valor=,)\n",
      "034. Token(tipo=T_IDENTIF, valor=c)\n",
      "035. Token(tipo=T_PUNCT, valor=))\n",
      "036. Token(tipo=T_KEYWORD, valor=if)\n",
      "037. Token(tipo=T_PUNCT, valor=()\n",
      "038. Token(tipo=T_IDENTIF, valor=c)\n",
      "039. Token(tipo=T_OP, valor=>)\n",
      "040. Token(tipo=T_IDENTIF, valor=a)\n",
      "041. Token(tipo=T_PUNCT, valor=))\n",
      "042. Token(tipo=T_PUNCT, valor={)\n",
      "043. Token(tipo=T_IDENTIF, valor=print)\n",
      "044. Token(tipo=T_PUNCT, valor=()\n",
      "045. Token(tipo=T_IDENTIF, valor=c)\n",
      "046. Token(tipo=T_PUNCT, valor=,)\n",
      "047. Token(tipo=T_STRING, valor=\" maior que \")\n",
      "048. Token(tipo=T_PUNCT, valor=,)\n",
      "049. Token(tipo=T_IDENTIF, valor=a)\n",
      "050. Token(tipo=T_PUNCT, valor=))\n",
      "051. Token(tipo=T_PUNCT, valor=})\n",
      "052. Token(tipo=T_KEYWORD, valor=elif)\n",
      "053. Token(tipo=T_PUNCT, valor=()\n",
      "054. Token(tipo=T_IDENTIF, valor=c)\n",
      "055. Token(tipo=T_OP, valor=<)\n",
      "056. Token(tipo=T_IDENTIF, valor=b)\n",
      "057. Token(tipo=T_PUNCT, valor=))\n",
      "058. Token(tipo=T_PUNCT, valor={)\n",
      "059. Token(tipo=T_IDENTIF, valor=print)\n",
      "060. Token(tipo=T_PUNCT, valor=()\n",
      "061. Token(tipo=T_IDENTIF, valor=c)\n",
      "062. Token(tipo=T_PUNCT, valor=,)\n",
      "063. Token(tipo=T_STRING, valor=\" menor que \")\n",
      "064. Token(tipo=T_PUNCT, valor=,)\n",
      "065. Token(tipo=T_IDENTIF, valor=b)\n",
      "066. Token(tipo=T_PUNCT, valor=))\n",
      "067. Token(tipo=T_PUNCT, valor=})\n",
      "068. Token(tipo=T_KEYWORD, valor=else)\n",
      "069. Token(tipo=T_PUNCT, valor={)\n",
      "070. Token(tipo=T_IDENTIF, valor=print)\n",
      "071. Token(tipo=T_PUNCT, valor=()\n",
      "072. Token(tipo=T_STRING, valor=\"error\")\n",
      "073. Token(tipo=T_PUNCT, valor=))\n",
      "074. Token(tipo=T_PUNCT, valor=})\n",
      "075. Token(tipo=T_IDENTIF, valor=c)\n",
      "076. Token(tipo=T_OP, valor==)\n",
      "077. Token(tipo=T_IDENTIF, valor=ternary)\n",
      "078. Token(tipo=T_PUNCT, valor=()\n",
      "079. Token(tipo=T_IDENTIF, valor=a)\n",
      "080. Token(tipo=T_PUNCT, valor=,)\n",
      "081. Token(tipo=T_IDENTIF, valor=b)\n",
      "082. Token(tipo=T_PUNCT, valor=))\n",
      "083. Token(tipo=T_IDENTIF, valor=print)\n",
      "084. Token(tipo=T_PUNCT, valor=()\n",
      "085. Token(tipo=T_IDENTIF, valor=c)\n",
      "086. Token(tipo=T_PUNCT, valor=))\n",
      "087. Token(tipo=T_KEYWORD, valor=var)\n",
      "088. Token(tipo=T_IDENTIF, valor=mercedes)\n",
      "089. Token(tipo=T_OP, valor==)\n",
      "090. Token(tipo=T_IDENTIF, valor=carro)\n",
      "091. Token(tipo=T_PUNCT, valor=()\n",
      "092. Token(tipo=T_STRING, valor=\"mercedes\")\n",
      "093. Token(tipo=T_PUNCT, valor=,)\n",
      "094. Token(tipo=T_STRING, valor=\"GLE\")\n",
      "095. Token(tipo=T_PUNCT, valor=,)\n",
      "096. Token(tipo=T_INT, valor=2022)\n",
      "097. Token(tipo=T_PUNCT, valor=,)\n",
      "098. Token(tipo=T_INT, valor=220)\n",
      "099. Token(tipo=T_PUNCT, valor=))\n",
      "100. Token(tipo=T_IDENTIF, valor=print)\n",
      "101. Token(tipo=T_PUNCT, valor=()\n",
      "102. Token(tipo=T_STRING, valor=\"O carro polui: \")\n",
      "103. Token(tipo=T_PUNCT, valor=,)\n",
      "104. Token(tipo=T_IDENTIF, valor=mercedes)\n",
      "105. Token(tipo=T_DOT, valor=None)\n",
      "106. Token(tipo=T_IDENTIF, valor=poluicao)\n",
      "107. Token(tipo=T_PUNCT, valor=))\n",
      "108. Token(tipo=T_SPECIAL, valor=end)\n",
      "109. Token(tipo=T_PUNCT, valor=})\n",
      "110. Token(tipo=T_KEYWORD, valor=func)\n",
      "111. Token(tipo=T_IDENTIF, valor=add)\n",
      "112. Token(tipo=T_PUNCT, valor=()\n",
      "113. Token(tipo=T_IDENTIF, valor=a)\n",
      "114. Token(tipo=T_PUNCT, valor=,)\n",
      "115. Token(tipo=T_IDENTIF, valor=b)\n",
      "116. Token(tipo=T_PUNCT, valor=))\n",
      "117. Token(tipo=T_PUNCT, valor={)\n",
      "118. Token(tipo=T_KEYWORD, valor=return)\n",
      "119. Token(tipo=T_IDENTIF, valor=a)\n",
      "120. Token(tipo=T_OP, valor=+)\n",
      "121. Token(tipo=T_IDENTIF, valor=b)\n",
      "122. Token(tipo=T_PUNCT, valor=})\n",
      "123. Token(tipo=T_KEYWORD, valor=func)\n",
      "124. Token(tipo=T_IDENTIF, valor=ternary)\n",
      "125. Token(tipo=T_PUNCT, valor=()\n",
      "126. Token(tipo=T_IDENTIF, valor=a)\n",
      "127. Token(tipo=T_PUNCT, valor=,)\n",
      "128. Token(tipo=T_IDENTIF, valor=b)\n",
      "129. Token(tipo=T_PUNCT, valor=))\n",
      "130. Token(tipo=T_PUNCT, valor={)\n",
      "131. Token(tipo=T_KEYWORD, valor=return)\n",
      "132. Token(tipo=T_IDENTIF, valor=a)\n",
      "133. Token(tipo=T_OP, valor===)\n",
      "134. Token(tipo=T_OP, valor==)\n",
      "135. Token(tipo=T_IDENTIF, valor=b)\n",
      "136. Token(tipo=T_CONDITIONAL_OP, valor=?)\n",
      "137. Token(tipo=T_SPECIAL, valor=true)\n",
      "138. Token(tipo=T_CONDITIONAL_OP, valor=:)\n",
      "139. Token(tipo=T_SPECIAL, valor=false)\n",
      "140. Token(tipo=T_PUNCT, valor=})\n",
      "141. Token(tipo=T_KEYWORD, valor=object)\n",
      "142. Token(tipo=T_IDENTIF, valor=carro)\n",
      "143. Token(tipo=T_PUNCT, valor=()\n",
      "144. Token(tipo=T_IDENTIF, valor=marca)\n",
      "145. Token(tipo=T_PUNCT, valor=,)\n",
      "146. Token(tipo=T_IDENTIF, valor=modelo)\n",
      "147. Token(tipo=T_PUNCT, valor=,)\n",
      "148. Token(tipo=T_IDENTIF, valor=ano)\n",
      "149. Token(tipo=T_PUNCT, valor=,)\n",
      "150. Token(tipo=T_IDENTIF, valor=velocidade)\n",
      "151. Token(tipo=T_PUNCT, valor=))\n",
      "152. Token(tipo=T_PUNCT, valor={)\n",
      "153. Token(tipo=T_KEYWORD, valor=init)\n",
      "154. Token(tipo=T_IDENTIF, valor=carro)\n",
      "155. Token(tipo=T_PUNCT, valor=()\n",
      "156. Token(tipo=T_IDENTIF, valor=marca)\n",
      "157. Token(tipo=T_PUNCT, valor=,)\n",
      "158. Token(tipo=T_IDENTIF, valor=modelo)\n",
      "159. Token(tipo=T_PUNCT, valor=,)\n",
      "160. Token(tipo=T_IDENTIF, valor=ano)\n",
      "161. Token(tipo=T_PUNCT, valor=,)\n",
      "162. Token(tipo=T_IDENTIF, valor=velocidade)\n",
      "163. Token(tipo=T_PUNCT, valor=))\n",
      "164. Token(tipo=T_KEYWORD, valor=func)\n",
      "165. Token(tipo=T_IDENTIF, valor=poluicao)\n",
      "166. Token(tipo=T_PUNCT, valor=()\n",
      "167. Token(tipo=T_IDENTIF, valor=velocidade)\n",
      "168. Token(tipo=T_PUNCT, valor=))\n",
      "169. Token(tipo=T_PUNCT, valor={)\n",
      "170. Token(tipo=T_KEYWORD, valor=return)\n",
      "171. Token(tipo=T_IDENTIF, valor=velocidade)\n",
      "172. Token(tipo=T_OP, valor=*)\n",
      "173. Token(tipo=T_INT, valor=10)\n",
      "174. Token(tipo=T_PUNCT, valor=})\n",
      "175. Token(tipo=T_PUNCT, valor=})\n",
      "\n",
      "Análise Sintática:\n",
      "Erro no parser: Erro de sintaxe no token # Bem vindo a essa nova linguagem de prog ANT\n",
      "\n",
      "Estados salvos: {'tokens': [Token(tipo=T_COMMENT, valor=# Bem vindo a essa nova linguagem de prog ANT), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_SPECIAL, valor=main), Token(tipo=T_IDENTIF, valor=home), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_INT, valor=2), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor==), Token(tipo=T_INT, valor=1), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_OP, valor==), Token(tipo=T_INT, valor=2), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=nome), Token(tipo=T_OP, valor==), Token(tipo=T_STRING, valor=\"Joao\"), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=add), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_STRING, valor=\"O resultado foi \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_KEYWORD, valor=if), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor=>), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_STRING, valor=\" maior que \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=elif), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor=<), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_STRING, valor=\" menor que \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=else), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_STRING, valor=\"error\"), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=ternary), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=mercedes), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=carro), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_STRING, valor=\"mercedes\"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_STRING, valor=\"GLE\"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_INT, valor=2022), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_INT, valor=220), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_STRING, valor=\"O carro polui: \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=mercedes), Token(tipo=T_DOT, valor=None), Token(tipo=T_IDENTIF, valor=poluicao), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_SPECIAL, valor=end), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=add), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=return), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor=+), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=ternary), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=return), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor===), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_CONDITIONAL_OP, valor=?), Token(tipo=T_SPECIAL, valor=true), Token(tipo=T_CONDITIONAL_OP, valor=:), Token(tipo=T_SPECIAL, valor=false), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=object), Token(tipo=T_IDENTIF, valor=carro), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=marca), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=modelo), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=ano), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=velocidade), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=init), Token(tipo=T_IDENTIF, valor=carro), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=marca), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=modelo), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=ano), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=velocidade), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=poluicao), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=velocidade), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=return), Token(tipo=T_IDENTIF, valor=velocidade), Token(tipo=T_OP, valor=*), Token(tipo=T_INT, valor=10), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_PUNCT, valor=})], 'symbol_table': {}}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filename = 'codigo2.x'\n",
    "    print(f\"Lendo o arquivo {filename} ...\")\n",
    "\n",
    "    arquivo = open(filename)\n",
    "    for l in arquivo.readlines():\n",
    "        l = l.replace('\\n','') # remove a quebra de linha\n",
    "        print(l)\n",
    "\n",
    "    # Tokenização\n",
    "    print(\"\\nTokenização:\")\n",
    "    tokens = tokenize()\n",
    "    print(\"Tokens:\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"{i+1:03}. {token}\")\n",
    "    \n",
    "    # Análise Sintática (Parser)\n",
    "    print(\"\\nAnálise Sintática:\")\n",
    "    parser = Parser(tokens)\n",
    "    try:\n",
    "        parser.start()\n",
    "        print(\"Análise sintática concluída com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no parser: {e}\")\n",
    "\n",
    "    states = {\n",
    "        'tokens': tokens,\n",
    "        'symbol_table': parser.symbol_table\n",
    "    }\n",
    "    print(\"\\nEstados salvos:\", states)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
