{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Token</h3>\n",
    "<h4>Analisador Lexico - AFD </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KEYWORD = \"<keyword %s>\"\n",
    "T_OP = \"<op %s>\"\n",
    "T_INT = \"<int %s>\"\n",
    "T_STRING = \"<string %s>\"\n",
    "T_IDENTIF = \"<id %s>\"\n",
    "T_SPECIAL = \"<special %s>\"\n",
    "T_PUNCT = \"<punct %s>\"\n",
    "T_DOT = \"<dot>\"\n",
    "T_CONDITIONAL_OP = \"<conditional_op %s>\"\n",
    "T_COMMENT = \"<comment>\"\n",
    "\n",
    "class Token():\n",
    "    def __init__(self, tipo, valor=None):\n",
    "        self.tipo = tipo\n",
    "        self.valor = valor\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.tipo}({self.valor})\" if self.valor else self.tipo\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "def tokenize_line(line, line_number):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    n = len(line)\n",
    "\n",
    "    while i < n:\n",
    "        if line[i].isspace():\n",
    "            i += 1\n",
    "        elif line[i] == '#':  # Comment\n",
    "            tokens.append(T_COMMENT)\n",
    "            break \n",
    "        elif line[i].isalpha():\n",
    "            start = i\n",
    "            while i < n and (line[i].isalnum() or line[i] == '_'):\n",
    "                i += 1\n",
    "            word = line[start:i]\n",
    "            if word in [\"var\", \"func\", \"if\", \"elif\", \"else\", \"return\", \"object\", \"init\"]:\n",
    "                tokens.append(Token(\"T_KEYWORD\", word))\n",
    "            elif word in [\"true\", \"false\", \"null\", \"end\", \"main\"]:\n",
    "                tokens.append(Token(\"T_SPECIAL\", word))\n",
    "            else:\n",
    "                tokens.append(Token(\"T_IDENTIF\", word))\n",
    "        elif line[i].isdigit():\n",
    "            start = i\n",
    "            while i < n and line[i].isdigit():\n",
    "                i += 1\n",
    "            tokens.append(Token(\"T_INT\", line[start:i]))\n",
    "        elif line[i] == '\"':\n",
    "            start = i\n",
    "            i += 1\n",
    "            while i < n and line[i] != '\"':\n",
    "                i += 1\n",
    "            if i >= n:\n",
    "                print(f\"Erro: String não fechada na linha {line_number}\")\n",
    "                raise StopExecution\n",
    "            i += 1\n",
    "            tokens.append(Token(\"T_STRING\", line[start:i]))\n",
    "        elif line[i] in \"=<>!+-*/\":\n",
    "            start = i\n",
    "            i += 1\n",
    "            if i < n and line[i] == \"=\":\n",
    "                i += 1\n",
    "            tokens.append(Token(\"T_OP\", line[start:i]))\n",
    "        elif line[i] in \"(),[]{}\":\n",
    "            tokens.append(Token(\"T_PUNCT\", line[i]))\n",
    "            i += 1\n",
    "        elif line[i] in \"?:\":\n",
    "            tokens.append(Token(\"T_CONDITIONAL_OP\", line[i]))\n",
    "            i += 1\n",
    "        elif line[i] == '.':\n",
    "            tokens.append(Token(\"T_DOT\"))\n",
    "            i += 1\n",
    "        else:\n",
    "            print(f\"Erro: Caractere não reconhecido '{line[i]}' na linha {line_number}\")\n",
    "            raise StopExecution\n",
    "    return tokens\n",
    "\n",
    "def tokenize():\n",
    "\n",
    "    try:\n",
    "        token_total = [];\n",
    "        with open('codigo.x', 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "\n",
    "        line_number = 0\n",
    "        for line in lines:\n",
    "            line_number += 1\n",
    "            tokens = tokenize_line(line, line_number)\n",
    "            token_total.extend(tokens)\n",
    "            #print(tokens)\n",
    "\n",
    "        return token_total\n",
    "\n",
    "    except StopExecution:\n",
    "        print(\"Execução parada devido a erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Analisador Sinatico - Parser </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, tipo, valor=None):\n",
    "        self.tipo = tipo\n",
    "        self.valor = valor\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.tipo}({self.valor})\" if self.valor else self.tipo\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = [Token(t.split(\" \")[0], t.split(\" \")[1] if len(t.split(\" \")) > 1 else None) for t in tokens]\n",
    "        self.pos = -1\n",
    "        self.token_atual = None\n",
    "        self.symbol_table = {}\n",
    "        self.proximo()\n",
    "\n",
    "    def proximo(self):\n",
    "        self.pos += 1\n",
    "        \n",
    "        if self.pos >= len(self.tokens):\n",
    "            self.token_atual = Token(\"EOF\")\n",
    "        else:    \n",
    "            self.token_atual = self.tokens[self.pos]\n",
    "\n",
    "        print(self.token_atual)\n",
    "        return self.token_atual\n",
    "\n",
    "    def erro(self):\n",
    "        raise Exception('Erro de sintaxe. %s' % (self.token_atual))\n",
    "\n",
    "    def use(self, tipo, valor=None):\n",
    "        if self.token_atual.tipo != tipo:\n",
    "            self.erro()\n",
    "        elif valor is not None and self.token_atual.valor != valor:\n",
    "            self.erro()\n",
    "        else:\n",
    "            self.proximo()\n",
    "\n",
    "    def expression(self):\n",
    "        return self.term()\n",
    "\n",
    "    def term(self):\n",
    "        return self.factor()\n",
    "\n",
    "    def factor(self):\n",
    "        if self.token_atual.tipo == \"T_INT\":\n",
    "            value = int(self.token_atual.valor)\n",
    "            self.use(\"T_INT\")\n",
    "            return value\n",
    "        elif self.token_atual.tipo == \"T_IDENTIF\":\n",
    "            value = self.token_atual.valor\n",
    "            self.use(\"T_IDENTIF\")\n",
    "            return value\n",
    "        elif self.token_atual.tipo == \"T_PUNCT\" and self.token_atual.valor == \"(\":\n",
    "            self.use(\"T_PUNCT\", \"(\")\n",
    "            value = self.expression()\n",
    "            self.use(\"T_PUNCT\", \")\")\n",
    "            return value\n",
    "        # ... adicione mais cláusulas aqui para outros tipos de fatores, como variáveis ou chamadas de funções\n",
    "\n",
    "    def statement(self):\n",
    "        if self.token_atual.tipo == \"T_IDENTIF\":\n",
    "            self.assignment_statement()\n",
    "        elif self.token_atual.tipo == \"T_KEYWORD\" and self.token_atual.valor in [\"if\", \"elif\", \"else\"]:\n",
    "            self.conditional_statement()\n",
    "        elif self.token_atual.tipo == \"T_KEYWORD\" and self.token_atual.valor in [\"for\", \"while\"]:\n",
    "            self.loop_statement()\n",
    "        elif self.token_atual.tipo == \"T_KEYWORD\" and self.token_atual.valor == \"func\":\n",
    "            self.function_definition()\n",
    "        elif self.token_atual.tipo == \"T_KEYWORD\" and self.token_atual.valor == \"object\":\n",
    "            self.object_definition()\n",
    "        # TODO: Adicione outros tipos de instruções conforme sua gramática\n",
    "\n",
    "    def assignment_statement(self):\n",
    "        var_name = self.token_atual.valor\n",
    "        self.use(\"T_IDENTIF\")\n",
    "        self.use(\"T_OP\", \"=\")\n",
    "        value = self.expression()  # Chamar a função de análise de expressão aqui\n",
    "        self.use(\"T_PUNCT\", \";\")\n",
    "        self.symbol_table[var_name] = value\n",
    "\n",
    "    def conditional_statement(self):\n",
    "        if self.token_atual.valor == \"if\":\n",
    "            self.use(\"T_KEYWORD\", \"if\")\n",
    "            self.use(\"T_PUNCT\", \"(\")\n",
    "            self.expression()  # Chamar a função de análise de expressão aqui\n",
    "            self.use(\"T_PUNCT\", \")\")\n",
    "            self.statement()\n",
    "        elif self.token_atual.valor == \"elif\":\n",
    "            self.use(\"T_KEYWORD\", \"elif\")\n",
    "            self.use(\"T_PUNCT\", \"(\")\n",
    "            self.expression()\n",
    "            self.use(\"T_PUNCT\", \")\")\n",
    "            self.statement()\n",
    "        elif self.token_atual.valor == \"else\":\n",
    "            self.use(\"T_KEYWORD\", \"else\")\n",
    "            self.statement()\n",
    "\n",
    "    def loop_statement(self):\n",
    "        # Implemente as instruções de loop (como for e while) aqui\n",
    "\n",
    "    def function_definition(self):\n",
    "        # Implemente a definição de função aqui\n",
    "\n",
    "    def object_definition(self):\n",
    "        # Implemente a definição do objeto aqui\n",
    "\n",
    "    def start(self):\n",
    "        # A função start deve ser o ponto de entrada para iniciar o parser\n",
    "        # Você deve implementar o fluxo de controle principal aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compilador</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo codigo.x...\n",
      "\n",
      "Tokenização:\n",
      "\n",
      "Análise Sintática:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gusta\\Desktop\\Lexer-compiler\\compilador.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     states \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m: tokens,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msymbol_table\u001b[39m\u001b[39m'\u001b[39m: parser\u001b[39m.\u001b[39msymbol_table\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEstados salvos:\u001b[39m\u001b[39m\"\u001b[39m, states)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m main()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# if __name__ == '__main__':\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m#     main()\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gusta\\Desktop\\Lexer-compiler\\compilador.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Análise Sintática (Parser)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAnálise Sintática:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m parser \u001b[39m=\u001b[39m Parser(tokens)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     parser\u001b[39m.\u001b[39mstart()\n",
      "\u001b[1;32mc:\\Users\\gusta\\Desktop\\Lexer-compiler\\compilador.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, tokens):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens \u001b[39m=\u001b[39m [Token(t\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m], t\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m] \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(t\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m tokens]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_atual \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gusta\\Desktop\\Lexer-compiler\\compilador.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, tokens):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens \u001b[39m=\u001b[39m [Token(t\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m], t\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(t\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gusta/Desktop/Lexer-compiler/compilador.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_atual \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Token' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filename = 'codigo.x'\n",
    "    print(f\"Lendo o arquivo {filename}...\")\n",
    "\n",
    "    # Tokenização\n",
    "    print(\"\\nTokenização:\")\n",
    "    tokens = tokenize()\n",
    "    \n",
    "    # Análise Sintática (Parser)\n",
    "    print(\"\\nAnálise Sintática:\")\n",
    "    parser = Parser(tokens)\n",
    "    try:\n",
    "        parser.start()\n",
    "        print(\"Análise sintática concluída com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no parser: {e}\")\n",
    "\n",
    "    states = {\n",
    "        'tokens': tokens,\n",
    "        'symbol_table': parser.symbol_table\n",
    "    }\n",
    "    print(\"\\nEstados salvos:\", states)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
